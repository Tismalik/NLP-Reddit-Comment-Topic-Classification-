{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Now let get to \"PREPROCESSING\" the Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perhaps he would, but it would still be becaus...</td>\n",
       "      <td>1</td>\n",
       "      <td>take her out recording appears to capture trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It’s also not a fair trial. Why can’t the repu...</td>\n",
       "      <td>1</td>\n",
       "      <td>take her out recording appears to capture trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil companies, pharmaceutical companies are wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>a senator wants to unilaterally release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>okay so i clicked on this expecting to hear th...</td>\n",
       "      <td>1</td>\n",
       "      <td>take her out recording appears to capture trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some states treat your house differently (Cast...</td>\n",
       "      <td>1</td>\n",
       "      <td>organized breakandenter gangs arrive as tourists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13985</th>\n",
       "      <td>&amp;gt; anti-Zionism is inherently antisemitic.\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>israel ambassador to us says radical left college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13986</th>\n",
       "      <td>Well, a lot of right-wing Israeli Jews (the on...</td>\n",
       "      <td>4</td>\n",
       "      <td>israel ambassador to us says radical left college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13987</th>\n",
       "      <td>I read the comment and didn't say anything abo...</td>\n",
       "      <td>-3</td>\n",
       "      <td>fox friends warns of muslim enclaves in the west</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>&amp;gt; Israel-Palestine debate giving people a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>israel ambassador to us says radical left college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>High School History isn't enough to go over th...</td>\n",
       "      <td>3</td>\n",
       "      <td>brazils fearful lgbt community prepares for a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13990 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  score  \\\n",
       "0      Perhaps he would, but it would still be becaus...      1   \n",
       "1      It’s also not a fair trial. Why can’t the repu...      1   \n",
       "2      Oil companies, pharmaceutical companies are wi...      1   \n",
       "3      okay so i clicked on this expecting to hear th...      1   \n",
       "4      Some states treat your house differently (Cast...      1   \n",
       "...                                                  ...    ...   \n",
       "13985  &gt; anti-Zionism is inherently antisemitic.\\n...      1   \n",
       "13986  Well, a lot of right-wing Israeli Jews (the on...      4   \n",
       "13987  I read the comment and didn't say anything abo...     -3   \n",
       "13988  &gt; Israel-Palestine debate giving people a s...      0   \n",
       "13989  High School History isn't enough to go over th...      3   \n",
       "\n",
       "                                               permalink  \n",
       "0        take her out recording appears to capture trump  \n",
       "1        take her out recording appears to capture trump  \n",
       "2                a senator wants to unilaterally release  \n",
       "3        take her out recording appears to capture trump  \n",
       "4       organized breakandenter gangs arrive as tourists  \n",
       "...                                                  ...  \n",
       "13985  israel ambassador to us says radical left college  \n",
       "13986  israel ambassador to us says radical left college  \n",
       "13987   fox friends warns of muslim enclaves in the west  \n",
       "13988  israel ambassador to us says radical left college  \n",
       "13989      brazils fearful lgbt community prepares for a  \n",
       "\n",
       "[13990 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Dataset into Dataframe\n",
    "df=pd.read_csv(\"redcom_1.csv\")\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 1: remove all moderator Bot comments \"I am a bot, and this action was performed automatically\"</h1>\n",
    "<h2>And remove duplicated row by checking in 'body' column</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malik\\miniconda3\\envs\\tensor2.5\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13816, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax to drop rows that contain a set of strings \n",
    "df = df[~df.body.str.contains('I am a bot')]\n",
    "\n",
    "# Removing Duplicates\n",
    "df.body.duplicated().sum()              #FInd and return no  of duplicates in a column\n",
    "df.loc[df.body.duplicated()]            #locate and display duplicates\n",
    "\n",
    "#Removes all rows of data with duplicates found in Body, (inplace) ensure the changes reflect on the DF \n",
    "df.drop_duplicates(subset=['body'], inplace=True)   \n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check manually\n",
    "#display(df)\n",
    "#df.to_csv(\"check_2.csv\", index=False)    #Save final Dataset into a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 2: Remove all Numbers, Punctuation and non-alphabet Characters, Additional spaces and Removal of 'Stop-words' </h1>\n",
    "<h2> Add lemmitization using WordNet Lemmatizer from NLTK</h2>\n",
    "<h2> Tokenize the words using Word_Tokenize from NLTK</h2>\n",
    "<h3> Note: We will not be stemming words in this project. Stemming sometime create unknown words(spelling Errors). </h3>\n",
    "<h3> We need a high degree of precision at every level so its best to use a Lemmatizer </h3>\n",
    "<h3> S.N: Vader can detect Case sensitive sentiment so its best not tamper with case sensitivity</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re                                       #regular expressions library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag, word_tokenize                #Both to be used in conjuction with lemmatizer function\n",
    "nltk.download('punkt')                              #needed for tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')         #needed for tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'tl', 'dr', 'drs', 'gt']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Building the Stopword List \"\"\"\n",
    "stop_words = stopwords.words('english')      #store downloaded stopword list in variable\n",
    "#print(stop_words)\n",
    "#print(len(stop_words))\n",
    "\n",
    "from string import punctuation      #import punctuation list\n",
    "\n",
    "# Add punctuations to stop word list\n",
    "for x in punctuation:\n",
    "    stop_words.append(x)\n",
    "\n",
    "\n",
    "#Additional stop word list\n",
    "more = ['tl', 'dr', 'drs', 'gt']\n",
    "for x in more:\n",
    "    stop_words.append(x)\n",
    "\n",
    "# More stopword in text document\n",
    "\"\"\"with open('stopwordlist.txt', 'r') as s:   #stopword list maybe too aggressive :(\n",
    "    for x in s:\n",
    "        stop_words.append(x.strip())\"\"\"\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add POS TAGGING FUCNTION </h1>\n",
    "<h2> reexamine data, consider spelling , consider Negators, Note: '. & ,' still present </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tag function works in conjuction with lemmatizer function, this is a rather difficult but necessary setup\n",
    "## Note: stemming is much easier to perform but ineffective for this usecase\n",
    "## See https://www.kaggle.com/alvations/basic-nlp-with-nltk for more details on workings\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. (Postag function)\"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "def lemmatize_sent(raw_text): \n",
    "    \"\"\"Lemmatize Function containing (POSTAG funtion) to be utilized by preprocess function \"\"\"\n",
    "    # Text input is string, returns lowercased strings.\n",
    "    # word_tokenize is the function from nltk that tokenized/splits our texts\n",
    "    # pos_tag function from NLTK that performs parts of speech assignment\n",
    "    # you can also add word.lower() as an args if necessary\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(raw_text))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Alternatetively for a Simpler stemming or lemmatization:: use the function below \\ndef preprocessor(raw_text):\\n    #\"\" Preprocessing Steps \"\"\\n    raw_text = str(raw_text)\\n    #raw_text = raw_text.lower()        #remove case sensitivity if neccesary\\n    \\n    #regular expression keeping only letters and reconstrusting some words\\n    raw_text = re.sub(\"[^A-Za-z]\", \" \", raw_text)\\n    raw_text = re.sub(r\"what\\'s\", \"what is \", raw_text)\\n    raw_text = re.sub(r\"\\'s\", \" is\", raw_text)\\n    raw_text = re.sub(r\"\\'ve\", \" have \", raw_text)\\n    raw_text = re.sub(r\"can\\'t\", \"cannot \", raw_text)\\n    raw_text = re.sub(r\"n\\'t\", \" not \", raw_text)\\n    raw_text = re.sub(r\"i\\'m\", \"i am \", raw_text)\\n    raw_text = re.sub(r\"\\'re\", \" are \", raw_text)\\n    raw_text = re.sub(r\"\\'d\", \" would \", raw_text)\\n    raw_text = re.sub(r\"\\'ll\", \" will \", raw_text)\\n\\n\\n\\n    # split words -> thus convert string into list ( \\'hello world\\' -> [\\'hello\\', \\'world\\'])\\n    words = raw_text.split()\\n\\n    cleaned_words = []\\n    wnl = WordNetLemmatizer()         #plug in here any other stemmer or lemmatiser you want to try out\\n    \\n    # remove stopwords\\n    for word in words:\\n        if word not in stop_words:\\n            cleaned_words.append(word)\\n    \\n    #Alternate method to stemm or lemmatise words easily\\n    lemmatized_words = []\\n    for word in cleaned_words:\\n        word = wnl.lemmatize(word)     #lemmatize the words if you are using a lemmatizer\\n        lemmatized_words.append(word)\\n    \\n    # converting list back to string\\n    return \" \".join(lemmatized_words)\\n   \\n   '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(raw_text):\n",
    "    \"\"\" Preprocessing Steps \"\"\"\n",
    "    raw_text = str(raw_text)\n",
    "    \n",
    "    #regular expression keeping only letters and addressing some negations \n",
    "    raw_text = re.sub(\"[^A-Za-z]\", \" \", raw_text)\n",
    "    raw_text = re.sub(r\"what's\", \"what is \", raw_text)\n",
    "    raw_text = re.sub(r\"aren't\", \"are not \", raw_text)\n",
    "    raw_text = re.sub(r\"wasn't\", \"was not \", raw_text)\n",
    "    raw_text = re.sub(r\"haven't\", \"have not \", raw_text)\n",
    "    raw_text = re.sub(r\"hasn't\", \"has not \", raw_text)\n",
    "    raw_text = re.sub(r\"hadn't\", \"had not \", raw_text)\n",
    "    raw_text = re.sub(r\"won't\", \"will not \", raw_text)\n",
    "    raw_text = re.sub(r\"wouldn't\", \"would not \", raw_text)\n",
    "    raw_text = re.sub(r\"don't\", \"do not \", raw_text)\n",
    "    raw_text = re.sub(r\"doesn't\", \"does not \", raw_text)\n",
    "    raw_text = re.sub(r\"didn't\", \"did not \", raw_text)\n",
    "    raw_text = re.sub(r\"can't\", \"can not \", raw_text)\n",
    "    raw_text = re.sub(r\"shouldn't\", \"should not \", raw_text)\n",
    "    raw_text = re.sub(r\"mightn't\", \"might not \", raw_text)\n",
    "    raw_text = re.sub(r\"mustn't\", \"must not \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'s\", \" is\", raw_text)\n",
    "    raw_text = re.sub(r\"\\'ve\", \" have \", raw_text)\n",
    "    raw_text = re.sub(r\"can't\", \"cannot \", raw_text)\n",
    "    raw_text = re.sub(r\"n't\", \" not \", raw_text)\n",
    "    raw_text = re.sub(r\"i'm\", \"i am \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'re\", \" are \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'d\", \" would \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'ll\", \" will \", raw_text)\n",
    "\n",
    "    return \" \".join([word for word in lemmatize_sent(raw_text) \n",
    "            if word not in stop_words])\n",
    "            #and not word.isdigit()   \n",
    "\n",
    "\"\"\"\n",
    "#Alternatetively for a Simpler stemming or lemmatization:: use the function below \n",
    "def preprocessor(raw_text):\n",
    "    #\"\" Preprocessing Steps \"\"\n",
    "    raw_text = str(raw_text)\n",
    "    #raw_text = raw_text.lower()        #remove case sensitivity if neccesary\n",
    "    \n",
    "    #regular expression keeping only letters and reconstrusting some words\n",
    "    raw_text = re.sub(\"[^A-Za-z]\", \" \", raw_text)\n",
    "    raw_text = re.sub(r\"what's\", \"what is \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'s\", \" is\", raw_text)\n",
    "    raw_text = re.sub(r\"\\'ve\", \" have \", raw_text)\n",
    "    raw_text = re.sub(r\"can't\", \"cannot \", raw_text)\n",
    "    raw_text = re.sub(r\"n't\", \" not \", raw_text)\n",
    "    raw_text = re.sub(r\"i'm\", \"i am \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'re\", \" are \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'d\", \" would \", raw_text)\n",
    "    raw_text = re.sub(r\"\\'ll\", \" will \", raw_text)\n",
    "\n",
    "\n",
    "\n",
    "    # split words -> thus convert string into list ( 'hello world' -> ['hello', 'world'])\n",
    "    words = raw_text.split()\n",
    "\n",
    "    cleaned_words = []\n",
    "    wnl = WordNetLemmatizer()         #plug in here any other stemmer or lemmatiser you want to try out\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    #Alternate method to stemm or lemmatise words easily\n",
    "    lemmatized_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = wnl.lemmatize(word)     #lemmatize the words if you are using a lemmatizer\n",
    "        lemmatized_words.append(word)\n",
    "    \n",
    "    # converting list back to string\n",
    "    return \" \".join(lemmatized_words)\n",
    "   \n",
    "   \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey sample sold review happen blah contain contract happen punctuation universal right right contain\n"
     ]
    }
   ],
   "source": [
    "#testing the function with a sample text#\n",
    "sample_text = \"Hey There! !!'!'!! This is a Sample SOLD'M review, which 2007 123happens {blah}%456 to contain CONTRACT happened punctuations universal rights of right contained.\"\n",
    "print(preprocess(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(preprocess)               # Apply preprocess to body column\n",
    "df['permalink'] = df['permalink'].apply(preprocess)     # Apply preprocess to permalink column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "df.to_csv(\"preprocessed_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13816, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END...."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "035b9c71e4a3a67db39a7728b05f0df5cc8b8760f61dc7a461f1625dbaae32b6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tensor2.5': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
